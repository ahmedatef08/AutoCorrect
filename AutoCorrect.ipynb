{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36190d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "                                             correct  \\\n",
      "0  سبحان الله ، الحكام العرب سيموت علي الكرسي ليظ...   \n",
      "1  النصر ات لا محال ان شاء الله . من يءمن بالله و...   \n",
      "2  الي كل شخص يعتقد ان بشار الاسد سوف يخرج من سور...   \n",
      "3  الاسد وعصابته - لحد الان - غير مستوعبين انه رح...   \n",
      "4  النظام الاسدي تجاوز حتي ما فعله معمر القذافي ف...   \n",
      "\n",
      "                                           incorrect  \n",
      "0  سبحان الله الحكام العرب سيموت علي الكرسي ليضهر...  \n",
      "1  النصر ات لا محال انشاء الله من يءمن بالله والي...  \n",
      "2  الي كل شخص يعتقد ان بشار الاسد سوف يخرج من سور...  \n",
      "3  الاسد وعصابته لحد الان غير مستوعبين ان و رح ين...  \n",
      "4  النظام الاسدي تجاوز حتي ما فعله معمر القذافي ف...  \n",
      "\n",
      "Column names:\n",
      "Index(['correct', 'incorrect'], dtype='object')\n",
      "\n",
      "Number of rows: 18350\n",
      "Missing values:\n",
      "correct      0\n",
      "incorrect    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Option 1: Using raw string with backslashes\n",
    "df = pd.read_csv(r\"C:\\Users\\Ahmed Atif\\Desktop\\nlp\\qalb-mini.csv\")\n",
    "\n",
    "# Option 2: Using forward slashes (also works on Windows)\n",
    "# df = pd.read_csv(\"C:/Users/Ahmed Atif/Desktop/nlp/qalb-mini.csv\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Print column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Print number of rows and check for missing values\n",
    "print(f\"\\nNumber of rows: {len(df)}\")\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1067dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size after cleaning: 18327\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop duplicate rows if any\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Rename columns to standard names (if needed)\n",
    "df.columns = ['wrong', 'correct']\n",
    "\n",
    "# Show dataset size after cleaning\n",
    "print(f\"\\nDataset size after cleaning: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb37a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wrong': 'سبحان الله ، الحكام العرب سيموت علي الكرسي ليظهر انه عنيد وقوي . لو كان بشار يحب ارضه او شعبه ، لخرج من الحكم شفقه ورحمه ببلد ضاع . هنا زال قناع هذا الرءيس ، الذي خيب ظن شعبه والشعوب المسلمه ، كل مال السورين نفق في شراء سلاح ليقتل به ، شتان وحكام اوربا ، الذين يتركون الكرسي لمجرد فتنه بسيطه لحبهم لبلدهم .', 'correct': 'سبحان الله الحكام العرب سيموت علي الكرسي ليضهر انه عنيد وقوي ، لوكان بشار يحب ارضه او شعبه لخرج من الحكم شفقه ورحمه ببلد ضاع ، هنا زال قناع هذا الرءيس اللذي خيب ظن شعبه والشعوب المسلمه ، كل مال السورين نفق في شراء سلاح ليقتل به ، شتانا وحكام اوربا الذين يتركون الكرسي لمجرد فتنه بسيطه لحبهم لبلدهم', '__index_level_0__': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas DataFrame to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Show a sample\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53de748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289e9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Load pre-trained tokenizer for Arabic → English (can adapt it to correction task)\n",
    "model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2629a9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a812542535704110992d1e3c589700f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize each example\n",
    "def preprocess_function(examples):\n",
    "    # Use 'wrong' as input and 'correct' as target\n",
    "    inputs = examples[\"wrong\"]\n",
    "    targets = examples[\"correct\"]\n",
    "    \n",
    "    # Tokenize input (source)\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Tokenize target (labels)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa87493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 90% train and 10% test\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = train_test[\"train\"]\n",
    "eval_dataset = train_test[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c275754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel\n",
    "\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d116f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./autocorrect-arabic-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e78164f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9f8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "107b002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Atif\\AppData\\Local\\Temp\\ipykernel_17276\\3119893157.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e8e1ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4124' max='4124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4124/4124 3:42:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.864100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.794600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.406900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.406100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62833]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4124, training_loss=0.7165021579550032, metrics={'train_runtime': 13339.0389, 'train_samples_per_second': 2.473, 'train_steps_per_second': 0.309, 'total_flos': 559119631122432.0, 'train_loss': 0.7165021579550032, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a218f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b577293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]  \n",
    "\n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"bleu\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc83b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Atif\\AppData\\Local\\Temp\\ipykernel_17276\\1114712184.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ace9fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 24:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "{'eval_loss': 0.330827534198761, 'eval_model_preparation_time': 0.004, 'eval_bleu': 0.5703066949140346, 'eval_runtime': 1511.195, 'eval_samples_per_second': 1.213, 'eval_steps_per_second': 0.152}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1045f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU score on test set: 0.5703\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBLEU score on test set: {eval_results['eval_bleu']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a8c5df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: انا ذاهب الى المدرسه\n",
      "Corrected: انا ذهب لمدرسه\n"
     ]
    }
   ],
   "source": [
    "def correct_sentence(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=64)\n",
    "    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected\n",
    "\n",
    "sample = \"انا ذاهب الى المدرسه\"\n",
    "print(\"\\nOriginal:\", sample)\n",
    "print(\"Corrected:\", correct_sentence(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "354c8bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟡 الجملة الأصلية: اخي العزيز ، انت تتمني ان تقوم ثورات بدول الخليج ، ولكن اقول لك : الشغله هذه بعيده عنك ، والسبب انهم عاءشون برغد ، والحمد لله ، وباحترام ، وهذا قد يكونوا محسودين عليه . ولكن تعرف اين مكان الثوره الصحيح ؟\n",
      "✅ التصحيح الحقيقي: اخي العزيز انت تتمني ان تقوم ثورات بدول الخليج ولكن اقولك شغله هذي بعيده ع\n",
      "🔵 تصحيح النموذج: اخي العزيز انت تتمني ان تقوم ثورات بدول الخليج ولكن اقول لك الشعله هذه بعيده\n",
      "\n",
      "🟡 الجملة الأصلية: الختيار كما يسميه البعض عند ربه ، ماذا قدم هذا المدعو عرفات ( اسمه الحقيقي القدوه ) في حياته ؟ قدم فلسطين وشعبها باسماء براقه ، وها هم اقاموا له صرحا من رخام و و و ، لو عرفوا حقيقته واصله لما عبدوه ،\n",
      "✅ التصحيح الحقيقي: الختيار كما يسميه البعض عند ربه ، ماذا قدم هذا المدعو عرفات ( اسمه الحقيقي الق\n",
      "🔵 تصحيح النموذج: الختيار كما يسميه البعض عند ربه ماذا قدم هذا المدعي اتراه ( اسمه الحقيقي القدو\n",
      "\n",
      "🟡 الجملة الأصلية: عار عليكم يا قاده الخزي والعار من عرب ومسلمين ان تتركوا غزه العظمي وحيده تواجه الارهاب الاسراءيلي . دول ارهابيه استعماريه لا انسانيه تدعم اسراءيل باسلحه واموال اغلبها من اصول عربيه . اما عظم\n",
      "✅ التصحيح الحقيقي: عار عليكم ياقاده الخزي والعار من عرب ومسلمين ان تتركوا غزه العظمي وحيده تواجه الارها\n",
      "🔵 تصحيح النموذج: عار عليكم ياقاده الخزي والعار من عرب ومسلمين ان تتركوا غزه العظمي وحيده تواجهد الار\n",
      "\n",
      "🟡 الجملة الأصلية: اود في هاته المداخله ان انوه ، بان الشعب الجزاءري - وككل الشعوب العربيه والغربيه - تحتاج الي حريات كامله . ونحن هنا لا نحتاج الي هذا العميل سعدي ، فكل اوراقه حرقت ؛ لذا اوجه له رساله : عد الي\n",
      "✅ التصحيح الحقيقي: اود في هاته المداخله ان انوه بان الشعب الجزاءري وككل الشعوب العربيه و الغربيه تحتاج الي\n",
      "🔵 تصحيح النموذج: اود في هاته المداخله ان انوه بان الشعب الجزاءري وككل الشعوب العربيه والغربيه تحتص الي\n",
      "\n",
      "🟡 الجملة الأصلية: انتبهوا ايها الساده ، ان فكره الاضراب علمانيه الطابع ، فلا تسمحوا لاحد بتسخيرها لمصالحه ، او ان يدس في داخلها التمييز الديني ، او لسرقه الاضواء من اصحابها ، والاخوان لا ينتمون الا لمصالحهم\n",
      "✅ التصحيح الحقيقي: انتبهوا ايها الساده ، ان فكره الاضراب علمانيه الطابع ، فلا تسمحوا لاحد بتسخيرها لم\n",
      "🔵 تصحيح النموذج: انتظروا ايها الساده ان فكره الاضراب علمانيه التبعر فلا تسمحوا لاحد بتحدمها لمصالحه\n",
      "\n",
      "🟡 الجملة الأصلية: حسبي الله هو ربي ونعم الوكيل ، اخواني المسلمين هذا هو الاختبار الثاني لكم . الاول سب النبي عليه الصلاه واتم التسليم من دول اوروبا ، والان تاتي ثاني الاهانات من اقوي مصدر تشريعي لديهم . . . . الي متي الاستسلام يا مسلمين\n",
      "✅ التصحيح الحقيقي: حسبي الله هو ربي ونعم الوكيل , اخواني المسلمين هذا هو الاختبار الثاني لكم الاولي\n",
      "🔵 تصحيح النموذج: حسبي الله هو ربي و نعم الوكيل اخواني المسلمين هذا هو التخابي الثاني لكم . الاول س\n",
      "\n",
      "🟡 الجملة الأصلية: اخواني اهل الايمان والحكمه ، لو ان هءلاء الشله المرتزقه ال الاحمر ، فان اليمن الي الاسوا لما لهم من تاريخ اسود في الفساد وجلب الفساد . انا اعرفهم جيدا ، يستخدمون الفتن لمصالحهم كاليهود \" فرق تسد \" . لا تصدقوهم لو تمرغوا\n",
      "✅ التصحيح الحقيقي: اخواني اهل الايمان والحكمه لوان هذوا الشله المرتزقه ال الاحمر فان اليمن الي الاسوء لما لهم من\n",
      "🔵 تصحيح النموذج: اخواني اهل الايمان والحكمه لو ان هءلاء الشله المرتزقه ال الاحمر فان اليمن الي الاسوا لما لهم\n",
      "\n",
      "🟡 الجملة الأصلية: سيدي العزيز ارجوك ان تفهم ما قال سيد البشر صلي الله عليه وسلم بقوله : \" لا يءمن احدكم حتي يحب لاخيه ما يحب لنفسه \" . لاخيه يعني لاخيه المءمن بالله واليوم الاخر ، اما هذا من تتكلم عنه ليس بمءمن\n",
      "✅ التصحيح الحقيقي: سيدي العزيز ارجوك ان تفهم ما قال سيد البشر صلي الله عليه وسلم بقوله : ( لا يءم\n",
      "🔵 تصحيح النموذج: سيدي العزيز ارجوك ان تفهم ما قال سيد البشر صلي الله عليه وسلم بقله لا يءمن اح\n",
      "\n",
      "🟡 الجملة الأصلية: ان المسلمين ليس لديهم عقبه للاتحاد سوي اسراءيل وايران ، ولكن ايران اقوي بحكم تلبسها بلباس الدين الاسلامي وهي الراعي الرسمي للميليشيات التي اخترقت العالم الاسلامي باسم الدين ، ولذلك يكذبون عندما يقولون سوف نحارب ايران ، لانهم ببساط\n",
      "✅ التصحيح الحقيقي: ان المسلمين ليس لديهم عقبه للاتحاد سوي اسراءيل وايران ولاكن ايران اقوي بحكم تلب\n",
      "🔵 تصحيح النموذج: ان المسلمين ليس لديهم قباده للتحاد سوي اسراءيل وايران ولكن ايران اقوي بحكم تل\n",
      "\n",
      "🟡 الجملة الأصلية: قريبا سنري النهايه المذله والمهينه لنظام الاسد الفاشي ، وسيذهب معه من سانده من العصابات المجرمه التي اتت من جنوب لبنان ، اما من اتوا من عاصمه الخوارج والبدع اقول لكم : دوركم ات لا محاله ، سيذيق\n",
      "✅ التصحيح الحقيقي: قريبا سنري النهايه المذله والمهينه لنظام الاسد الفاشي , وسيذهب معه من سانده من العصابات الم\n",
      "🔵 تصحيح النموذج: قريبا سنري النهايه المذله والمهينه لنظام الاسد الفاشي وسيذهب معه من سانده من العصابات المجر\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    input_text = eval_dataset[i]['input_ids']\n",
    "    input_decoded = tokenizer.decode(input_text, skip_special_tokens=True)\n",
    "\n",
    "    label_text = eval_dataset[i]['labels']\n",
    "    label_decoded = tokenizer.decode(label_text, skip_special_tokens=True)\n",
    "\n",
    "    generated = model.generate(tokenizer.encode(input_decoded, return_tensors=\"pt\"))\n",
    "    prediction = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n🟡 الجملة الأصلية: {input_decoded}\")\n",
    "    print(f\"✅ التصحيح الحقيقي: {label_decoded}\")\n",
    "    print(f\"🔵 تصحيح النموذج: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6109f139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Ahmed Atif\\\\Desktop\\\\APIs2/autocorrect-arabic-model\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Ahmed Atif\\\\Desktop\\\\APIs2/autocorrect-arabic-model\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Ahmed Atif\\\\Desktop\\\\APIs2/autocorrect-arabic-model\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Ahmed Atif\\\\Desktop\\\\APIs2/autocorrect-arabic-model\\\\source.spm',\n",
       " 'C:\\\\Users\\\\Ahmed Atif\\\\Desktop\\\\APIs2/autocorrect-arabic-model\\\\target.spm',\n",
       " 'C:\\\\Users\\\\Ahmed Atif\\\\Desktop\\\\APIs2/autocorrect-arabic-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(r\"C:\\Users\\Ahmed Atif\\Desktop\\APIs2/autocorrect-arabic-model\")\n",
    "tokenizer.save_pretrained(r\"C:\\Users\\Ahmed Atif\\Desktop\\APIs2/autocorrect-arabic-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139dc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
